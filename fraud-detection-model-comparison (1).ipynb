{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23498,"sourceType":"datasetVersion","datasetId":310}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Libraries for data manipulation\nimport pandas as pd \nimport numpy as np\n# Libraries for test-train split & deploying isolation forest\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.metrics import classification_report, confusion_matrix\n# Libraries for viz\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Libraries for autoencoder\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\nfrom keras import regularizers\nfrom keras.optimizers import Adam\n# Supress warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-19T19:28:17.749875Z","iopub.execute_input":"2024-03-19T19:28:17.750281Z","iopub.status.idle":"2024-03-19T19:28:32.433331Z","shell.execute_reply.started":"2024-03-19T19:28:17.750241Z","shell.execute_reply":"2024-03-19T19:28:32.432165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will deploy and try to understand the methods while eventually comparing results of different chosen approaches. We will be using GPT 4 for guidance to choose and apply the different approaches given our problem statement. \n**Our objective will be to maximise the recall value with a high precision value.**","metadata":{}},{"cell_type":"markdown","source":"# 1. Data Pre-processing","metadata":{}},{"cell_type":"code","source":"# Reading in the data\ndata = pd.read_csv(\"/kaggle/input/creditcardfraud/creditcard.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T19:28:32.435071Z","iopub.execute_input":"2024-03-19T19:28:32.435696Z","iopub.status.idle":"2024-03-19T19:28:36.962694Z","shell.execute_reply.started":"2024-03-19T19:28:32.435666Z","shell.execute_reply":"2024-03-19T19:28:36.961861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A glance at the data\ndata.describe()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T19:28:36.964093Z","iopub.execute_input":"2024-03-19T19:28:36.964776Z","iopub.status.idle":"2024-03-19T19:28:37.418029Z","shell.execute_reply.started":"2024-03-19T19:28:36.964739Z","shell.execute_reply":"2024-03-19T19:28:37.416952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A look at the predictor variable\ndata[\"Class\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T19:28:37.420191Z","iopub.execute_input":"2024-03-19T19:28:37.420697Z","iopub.status.idle":"2024-03-19T19:28:37.435035Z","shell.execute_reply.started":"2024-03-19T19:28:37.420663Z","shell.execute_reply":"2024-03-19T19:28:37.43393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A look at the shape of the data\noriginal_rows = len(data)\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-19T19:28:37.436169Z","iopub.execute_input":"2024-03-19T19:28:37.436497Z","iopub.status.idle":"2024-03-19T19:28:37.445678Z","shell.execute_reply.started":"2024-03-19T19:28:37.436472Z","shell.execute_reply":"2024-03-19T19:28:37.444738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.1 Removing Duplicate Values","metadata":{}},{"cell_type":"code","source":"# Removing duplicate values \ndata.drop_duplicates(subset = None, keep = \"first\", inplace = True, ignore_index = True)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T19:28:37.446759Z","iopub.execute_input":"2024-03-19T19:28:37.447055Z","iopub.status.idle":"2024-03-19T19:28:38.243756Z","shell.execute_reply.started":"2024-03-19T19:28:37.447032Z","shell.execute_reply":"2024-03-19T19:28:38.242475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Again having a look at the shape of the data to check the number of removed rows\ndedup_rows = len(data)\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-19T19:28:38.245214Z","iopub.execute_input":"2024-03-19T19:28:38.245733Z","iopub.status.idle":"2024-03-19T19:28:38.253321Z","shell.execute_reply.started":"2024-03-19T19:28:38.245698Z","shell.execute_reply":"2024-03-19T19:28:38.252223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Total rows removed \nprint(\"Total duplicate rows removed : \", original_rows -dedup_rows)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T19:28:38.254569Z","iopub.execute_input":"2024-03-19T19:28:38.254988Z","iopub.status.idle":"2024-03-19T19:28:38.262982Z","shell.execute_reply.started":"2024-03-19T19:28:38.25496Z","shell.execute_reply":"2024-03-19T19:28:38.261811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A look at the predictor variable\ndata[\"Class\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T19:28:38.264238Z","iopub.execute_input":"2024-03-19T19:28:38.264617Z","iopub.status.idle":"2024-03-19T19:28:38.278144Z","shell.execute_reply.started":"2024-03-19T19:28:38.264583Z","shell.execute_reply":"2024-03-19T19:28:38.277121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 Variable Correlation","metadata":{}},{"cell_type":"code","source":"# Looking at the variable correlations\ncorr= round(data.corr(),2)\nplt.figure(figsize=(12,8))\nsns.set(font_scale=0.8)\nsns.heatmap(corr, cmap = 'viridis', annot = True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T19:28:38.282274Z","iopub.execute_input":"2024-03-19T19:28:38.282911Z","iopub.status.idle":"2024-03-19T19:28:41.611211Z","shell.execute_reply.started":"2024-03-19T19:28:38.282876Z","shell.execute_reply":"2024-03-19T19:28:41.6104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visually inspecting the impact of Amount over frauds\nsns.set_style(style='dark')\nsns.FacetGrid(data=data, col='Class').map(sns.scatterplot, 'Time', 'Amount', palette='muted')","metadata":{"execution":{"iopub.status.idle":"2024-03-19T19:28:43.569223Z","shell.execute_reply.started":"2024-03-19T19:28:41.612996Z","shell.execute_reply":"2024-03-19T19:28:43.568475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 Variable Selection","metadata":{}},{"cell_type":"code","source":"# As there doesn't seem to be any impact of Amount & Time over frauds we're removing both variables from our datasets\nnew_data = data.drop([\"Time\",\"Amount\"], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T19:28:43.570546Z","iopub.execute_input":"2024-03-19T19:28:43.57139Z","iopub.status.idle":"2024-03-19T19:28:43.600572Z","shell.execute_reply.started":"2024-03-19T19:28:43.571361Z","shell.execute_reply":"2024-03-19T19:28:43.599466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data split to test and train \nX = new_data.drop(\"Class\", axis = 1)\ny = new_data[\"Class\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T19:28:43.602032Z","iopub.execute_input":"2024-03-19T19:28:43.602361Z","iopub.status.idle":"2024-03-19T19:28:43.802947Z","shell.execute_reply.started":"2024-03-19T19:28:43.602334Z","shell.execute_reply":"2024-03-19T19:28:43.80184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Isolation Trees\n\nIsolation Trees (iTrees) are a type of algorithm primarily used for anomaly detection. They work by isolating observations, assuming that anomalies are easier to isolate compared to normal points due to their fewer numbers and distinct attribute values. An iTree recursively partitions the data by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. This process continues until the points are isolated or a limit in tree depth is reached. Anomalies tend to have shorter paths in the tree, indicating they are easier to isolate.\n\n![](http://https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fcf1c1d99-47bb-4a34-aec5-3431a929335f%2FUntitled.png?table=block&id=79c1ba86-2c87-4f38-8c7f-c496f1763aaf&cache=v2)\n\nThis is a highly imbalanced dataset and it makes perfect sense to apply Isolation trees as :\n1. the algorithm focusses on isolating anomalies\n2. they work well with high dimension data and don't suffer from the curse of dimensionality\n3. they use random features to work with which makes them highly scalable and efficient in terms of required computational power","metadata":{}},{"cell_type":"code","source":"tree = [10, 25, 50, 100, 150, 200, 250, 500,1000]\nfor value in tree :\n    iso_forest = IsolationForest(n_estimators=value, contamination='auto', random_state=42)\n    iso_forest.fit(X_train)\n    test_predictions = iso_forest.predict(X_test)\n    test_binary_predictions = np.where(test_predictions == -1, 1, 0)\n    print(\"No of trees used for prediction \",value, \"\\n\", confusion_matrix(y_test, test_binary_predictions), \"\\n\", classification_report(y_test, test_binary_predictions))","metadata":{"execution":{"iopub.status.busy":"2024-03-19T19:28:43.804701Z","iopub.execute_input":"2024-03-19T19:28:43.805187Z","iopub.status.idle":"2024-03-19T19:29:29.666317Z","shell.execute_reply.started":"2024-03-19T19:28:43.805158Z","shell.execute_reply":"2024-03-19T19:29:29.665528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Upon analyzing we observe that we get the **best results with 50 trees where we get a recall value of 80% and precision of around 5%**\n\nGoing forward we would like to improve upon the recall value first of all and then focus on the precision value","metadata":{}},{"cell_type":"markdown","source":"# 3. Autoencoders\n\nAutoencoders are trained to compress (encode) the input data into a lower-dimensional representation and then reconstruct (decode) it back to the original input. By training exclusively on normal (non-fraudulent) transactions, the autoencoder learns to capture the typical patterns of normal behavior.\n\nWhen a new transaction is input into the trained autoencoder, if the transaction is normal, the autoencoder should be able to reconstruct it well, resulting in a low reconstruction error. However, if the transaction is fraudulent (thus differing from the normal pattern it has learned), the reconstruction error will be high, signaling a potential anomaly.\n\n## 3.1 Data Creation for Auto Encoders","metadata":{}},{"cell_type":"code","source":"# Reset index to make index a column\nX_train_reset = X_train.reset_index()\ny_train_reset = y_train.reset_index()\n\n# Now merge using 'index' as a column\nmerged_df = X_train_reset.merge(y_train_reset, on='index', how='left')\n\n# Creatig copy of merged_df\nX_train_normal = merged_df.copy()\n\n# Selecting only non-fraudulent transactions\nX_train_normal = X_train_normal[X_train_normal[\"Class\"] == 0]\n\n# Dropping the column Class \nX_train_normal = X_train_normal.drop(columns = [\"Class\", \"index\"])","metadata":{"execution":{"iopub.status.busy":"2024-03-19T19:29:29.667434Z","iopub.execute_input":"2024-03-19T19:29:29.667742Z","iopub.status.idle":"2024-03-19T19:29:29.846691Z","shell.execute_reply.started":"2024-03-19T19:29:29.667717Z","shell.execute_reply":"2024-03-19T19:29:29.845737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_normal.describe()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T19:29:29.847777Z","iopub.execute_input":"2024-03-19T19:29:29.848053Z","iopub.status.idle":"2024-03-19T19:29:30.176804Z","shell.execute_reply.started":"2024-03-19T19:29:29.848031Z","shell.execute_reply":"2024-03-19T19:29:30.175728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe that the variables have values between -100 to 100 and hence before making any decision we would like to see the distribution of the variables","metadata":{}},{"cell_type":"markdown","source":"## 3.2 Variable Distribution","metadata":{}},{"cell_type":"code","source":"for col in X_train_normal.columns:\n    plt.figure(figsize=(10, 6))  # Specify your desired figure size\n    sns.histplot(data=X_train_normal[col], kde=True)\n    plt.title(f'Distribution of {col}')  # Optional: Adds a title to each plot\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T19:29:30.178432Z","iopub.execute_input":"2024-03-19T19:29:30.178753Z","iopub.status.idle":"2024-03-19T19:31:27.486498Z","shell.execute_reply.started":"2024-03-19T19:29:30.178729Z","shell.execute_reply":"2024-03-19T19:31:27.485381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that all variables are normally distributed hence we can go ahead with z-scale normalisation","metadata":{}},{"cell_type":"markdown","source":"## 3.3 Variable Scaling","metadata":{}},{"cell_type":"code","source":"# Initialize the StandardScaler\nscaler = StandardScaler()\n\n# Fit the scaler to your data and transform it\nX_train_normal_scaled = scaler.fit_transform(X_train_normal)\n\n# Convert the scaled data back to a DataFrame (optional, for convenience)\nX_train_normal_scaled = pd.DataFrame(X_train_normal_scaled, columns=X_train_normal.columns)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T19:31:27.488372Z","iopub.execute_input":"2024-03-19T19:31:27.48916Z","iopub.status.idle":"2024-03-19T19:31:27.585967Z","shell.execute_reply.started":"2024-03-19T19:31:27.489122Z","shell.execute_reply":"2024-03-19T19:31:27.584598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.4 Algorithm Application","metadata":{}},{"cell_type":"code","source":"learning_rates = [0.001, 0.0001, 0.00001]  # Define learning rates to test\nthreshold_percentiles = [88, 92, 97]  # Define percentile values for setting thresholds\n\n# Initialize the StandardScaler\nscaler = StandardScaler()\n# Assuming X_test is your test dataset\nX_test_scaled = scaler.fit_transform(X_test)\nX_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n\nfor lr in learning_rates:\n    # Define the architecture\n    input_dim = X_train_normal_scaled.shape[1]\n    input_layer = Input(shape=(input_dim,))\n    encoded = Dense(64, activation='relu')(input_layer)\n    encoded = Dense(32, activation='relu')(encoded)\n    decoded = Dense(64, activation='relu')(encoded)\n    decoded = Dense(input_dim, activation='sigmoid')(decoded)\n\n    # Compile the autoencoder with the current learning rate\n    autoencoder = Model(input_layer, decoded)\n    autoencoder.compile(optimizer=Adam(learning_rate=lr), loss='mean_squared_error')\n\n    # Train the autoencoder\n    autoencoder.fit(X_train_normal_scaled, X_train_normal_scaled,\n                    epochs=100,\n                    batch_size=256,\n                    shuffle=True,\n                    validation_split=0.2)\n\n    # Predict on the test set\n    reconstructed = autoencoder.predict(X_test_scaled)\n\n    # Calculate MSE for each instance\n    mse = np.mean(np.square(X_test_scaled - reconstructed), axis=1)\n\n    for percentile in threshold_percentiles:\n        # Determine a threshold for anomaly detection\n        threshold = np.percentile(mse, percentile)  # Set threshold based on the defined percentile of MSE\n\n        # Detect anomalies\n        anomalies = mse > threshold\n\n        # Evaluate the model\n        print(f\"Results for learning rate: {lr} and threshold percentile: {percentile}\")\n        print(confusion_matrix(y_test, anomalies))\n        print(classification_report(y_test, anomalies))","metadata":{"execution":{"iopub.status.busy":"2024-03-19T19:31:27.587396Z","iopub.execute_input":"2024-03-19T19:31:27.587815Z","iopub.status.idle":"2024-03-19T19:40:16.778823Z","shell.execute_reply.started":"2024-03-19T19:31:27.587778Z","shell.execute_reply":"2024-03-19T19:40:16.777642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there is a minor improvement using an autoencode. This is a WIP and we will use more values for the optimizers and try to explain autoencoders and the steps with more details. \n\nTo be continued !","metadata":{}}]}